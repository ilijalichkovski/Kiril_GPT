{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1UkJYSaF70w",
        "outputId": "4668265e-364d-4c3d-b3e6-fb0ae4da1420"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmBkEp-oXlB2"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0xZUya_ETPf",
        "outputId": "7f29b277-307d-47c4-ca5f-f1b97a32bb15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import mmap\n",
        "import random\n",
        "import tqdm\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "\n",
        "block_size = 1024      # sequence length\n",
        "batch_size = 32       # how many blocks simultaneously\n",
        "max_iters = 1000      # how many iterations of training\n",
        "learning_rate = 3e-3  # step size in updating weights during gradient descent\n",
        "eval_iters = 250      # interval to report loss\n",
        "n_embd = 384          # embedding dimension\n",
        "n_head = 4            # number of heads in multi-head attention\n",
        "n_layer = 4           # how many decoder layers\n",
        "dropout = 0.1         # 10% of total neurons are dropped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJJdPWh4EaN1",
        "outputId": "fb949869-11a3-42f9-fd8a-ddfd25ae1c79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['\\t', '\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '\\xa0', '§', '«', '°', '±', '²', '³', 'µ', '·', '»', '½', 'Á', 'É', 'Ë', 'Ö', '×', 'Ü', 'ß', 'à', 'á', 'â', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'ï', 'ñ', 'ò', 'ó', 'õ', 'ö', 'ú', 'ü', 'Ā', 'ā', 'ą', 'ć', 'č', 'Ē', 'ē', 'ę', 'ğ', 'Ī', 'ī', 'ı', 'Ł', 'ł', 'ń', 'Ō', 'ō', 'ő', 'Ś', 'ś', 'Ş', 'ş', 'Š', 'š', 'Ū', 'ū', 'ź', 'ż', 'Ž', 'ž', 'ǐ', 'ə', 'ɛ', 'ɫ', '́', '΄', 'Ά', 'ΐ', 'Α', 'Β', 'Γ', 'Δ', 'Ε', 'Η', 'Θ', 'Ι', 'Κ', 'Λ', 'Μ', 'Ν', 'Ο', 'Π', 'Ρ', 'Σ', 'Τ', 'Φ', 'Χ', 'Ψ', 'ά', 'έ', 'ή', 'ί', 'α', 'β', 'γ', 'δ', 'ε', 'ζ', 'η', 'θ', 'ι', 'κ', 'λ', 'μ', 'ν', 'ξ', 'ο', 'π', 'ρ', 'ς', 'σ', 'τ', 'υ', 'φ', 'χ', 'ψ', 'ω', 'ό', 'ύ', 'ώ', 'Ђ', 'Ѓ', 'Є', 'Ѕ', 'І', 'Ј', 'Љ', 'Њ', 'Ћ', 'Ќ', 'Џ', 'А', 'Б', 'В', 'Г', 'Д', 'Е', 'Ж', 'З', 'И', 'Й', 'К', 'Л', 'М', 'Н', 'О', 'П', 'Р', 'С', 'Т', 'У', 'Ф', 'Х', 'Ц', 'Ч', 'Ш', 'Щ', 'Ъ', 'Ы', 'Ь', 'Э', 'Ю', 'Я', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я', 'ѐ', 'ђ', 'ѓ', 'є', 'ѕ', 'і', 'ї', 'ј', 'љ', 'њ', 'ћ', 'ќ', 'ѝ', 'џ', 'Ѣ', 'Ѳ', 'қ', 'ү', 'ә', 'א', 'ב', 'ה', 'ו', 'ז', 'י', 'ל', 'ן', 'ס', 'آ', 'إ', 'ا', 'ب', 'ة', 'ج', 'ح', 'خ', 'د', 'ر', 'ز', 'س', 'ض', 'ع', 'غ', 'ف', 'ق', 'ل', 'م', 'ن', 'ه', 'و', 'ي', 'پ', 'ک', 'க', 'த', 'ப', 'வ', 'ி', '்', 'ഗ', 'ത', 'ി', 'ḇ', 'ḡ', 'ớ', 'ừ', 'Ἡ', 'ἱ', 'Ὠ', 'ὰ', 'ὸ', 'ῆ', '\\u200e', '–', '—', '―', '‘', '’', '“', '”', '„', '†', '•', '\\u202f', '‰', '€', '←', '↑', '→', '↓', '⇌', '−', '∞', '≈', '▲', '△', '♀', '♂', '✔', 'か', 'ら', 'ア', 'ク', 'コ', 'ニ', 'メ', 'リ', '一', '中', '二', '人', '共', '劇', '务', '十', '华', '和', '国', '坂', '場', '届', '接', '民', '版', '第', '置', '詞', '院']\n"
          ]
        }
      ],
      "source": [
        "# dataloader\n",
        "\n",
        "chars = ''\n",
        "with open('/content/drive/MyDrive/Datasets/mk_vocab_1.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "    chars = sorted(list(set(text)))\n",
        "\n",
        "print(chars)\n",
        "vocab_size = len(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTIyMp4EEbJZ",
        "outputId": "f0baacb4-268e-4329-9681-3bd0784c9485"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([ 34,   1, 363,   1, 338,   1, 307,   1, 279,   1, 158,   1, 159,   1,\n",
            "        147,   1, 361,   1, 251,   1, 205,   1,  90,   1, 400,   1, 381,   1,\n",
            "         15,   1, 222,   1, 170,   1, 335,   1, 296,   1, 104,   1,  53,   1,\n",
            "        173,   1, 391,   1,  64,   1, 264,   1,  50,   1, 288,   1, 230,   1,\n",
            "        181,   1, 377,   1,  95,   1, 260,   1, 359,   1, 421,   1, 374,   1,\n",
            "         20,   1, 426,   1, 180,   1,   9,   1, 265,   1, 324,   1, 305,   1,\n",
            "        417,   1, 125,   1, 185,   1,  68,   1, 152,   1, 270,   1,  70,   1,\n",
            "        275,   1])\n"
          ]
        }
      ],
      "source": [
        "# tokenizer\n",
        "\n",
        "string_to_int = { ch:i for i,ch in enumerate(chars) }\n",
        "int_to_string = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [string_to_int[c] for c in s]\n",
        "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MHe-bKYUIf-0"
      },
      "outputs": [],
      "source": [
        "# memory map\n",
        "def get_random_chunk(split):\n",
        "    filename = \"/content/drive/MyDrive/Datasets/train_mk_1.txt\" if split == 'train' else \"/content/drive/MyDrive/Datasets/val_mk_1.txt\"\n",
        "    with open(filename, 'rb') as f: # 'rb' reads the file in binary\n",
        "        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
        "            # determine the file size and a random position that's early enough not to go out of bounds\n",
        "            file_size = len(mm)\n",
        "            start_pos = random.randint(0, (file_size) - block_size*batch_size)\n",
        "\n",
        "            # seek to the random position and read from there onward\n",
        "            mm.seek(start_pos)\n",
        "            block=mm.read(block_size*batch_size-1)\n",
        "\n",
        "            # convert binary to utf-8 string\n",
        "            decoded_block = block.decode('utf-8', errors='ignore').replace('\\r', '')\n",
        "\n",
        "            # encode it and save it as a tensor\n",
        "            data = torch.tensor(encode(decoded_block), dtype=torch.long)\n",
        "\n",
        "    return data\n",
        "\n",
        "def get_batch(split):\n",
        "    data = get_random_chunk(split)\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8Oo88rUYEaIr"
      },
      "outputs": [],
      "source": [
        "# loss estimation\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval() # in this mode, model turns off dropout and batch norm layers\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train() # in this mode, batch normalization and dropout are activated\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Pnooox-EEaEL"
      },
      "outputs": [],
      "source": [
        "# Language Model!\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\"dot product attention head\"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key   = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # it simply registers the 'dont look ahead' masking in the state of the model. so we dont compute it every time -- saves time!\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #  input is size (batch, time-step, channels)\n",
        "        # output is size (batch, time-step, head size)\n",
        "\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)  # (B,T,hs)\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "\n",
        "        # scaled dot product attention!\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B,T,hs) @ (B,hs,T) = (B,T,T)\n",
        "        # tril\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B,T,T)\n",
        "        #softmax\n",
        "        wei = F.softmax(wei, dim=-1) # (B,T,T)\n",
        "        wei = self.dropout(wei)\n",
        "        # matmul\n",
        "        v = self.value(x) # (B,T,hs)\n",
        "        out = wei @ v # (B,T,T) @ (B,T,hs) = (B,T,hs)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"multple parallel heads of attention\"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)]) # create the heads. ModuleList simply holds the modules in a list lol\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd) # projects head_size*num_heads to n_embd. it's already the case but just in case\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1) # (B,T,[h1, h1, h1, h1, h2, h2, h2, h2....])\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"linear -> ReLU --> linear\"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd), #inner dimensions of linear layers match for multiplicative compatibility\n",
        "            nn.Dropout(dropout),\n",
        "            )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"decoder block\"\"\"\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head # // is floor division\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.sa(x)\n",
        "        x = self.ln1(x + y) # this is post-norm (add + norm)\n",
        "        y = self.ffwd(x)\n",
        "        x = self.ln2(x + y) # also post-norm (add + norm)\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "\n",
        "        # first, we have the token and positional embeddings\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)     # token embeddings\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)  # positional embeddings\n",
        "\n",
        "        # then, the 4 decoder blocks connected sequentially\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)]) # 4 decoder layers\n",
        "\n",
        "        # then, linear transformation\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        # initialize our weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, index, targets=None):\n",
        "        B, T = index.shape\n",
        "\n",
        "        # indices and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(index)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) #(B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, index, max_new_tokens):\n",
        "        # index is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions and loss using the forward func\n",
        "            logits, loss = self.forward(index)\n",
        "            # focus on the last step of the T (time) dimension\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get the probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence unti the loop ends\n",
        "            index = torch.cat((index, index_next), dim=1) #\n",
        "\n",
        "        return index\n",
        "\n",
        "model = GPTLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "\n",
        "#context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "#generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
        "#print(generated_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "6sh0SzBlEZ1P",
        "outputId": "004bcd0c-04a0-40ae-ed80-625828cf47d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step: 0, train loss: 6.1425, val loss: 6.1386\n",
            "step: 250, train loss: 3.8254, val loss: 3.8709\n",
            "step: 500, train loss: 3.8246, val loss: 3.8427\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-ffce7ef5e2c0>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0meval_iters\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'step: {iter}, train loss: {losses[\"train\"]:.4f}, val loss: {losses[\"val\"]:.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-50854ad35b01>\u001b[0m in \u001b[0;36mestimate_loss\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# in this mode, batch normalization and dropout are activated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in tqdm(range(max_iters)):\n",
        "\n",
        "    if iter % eval_iters == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f'step: {iter}, train loss: {losses[\"train\"]:.4f}, val loss: {losses[\"val\"]:.4f}')\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model.forward(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAH_sqO9FsWI",
        "outputId": "d448716f-fdb1-45ad-c717-3e55cbeef736"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "како  ткуin в \n",
            "8о гг а> ко1'с\n",
            "овосј кз9\n",
            "t,с т!ссИ.нп0i69ев-|  и  4 о mе 3вцре ани9д1аеи  р 0л|  Снер5  Блр 8отиолАехс3сееs|u9аееаa8|а1=аааре  д a'о Втhs|анрв=пaосба8сxикпGолти>тт oч|emzв\n",
            "а (a3јсодстоав\n",
            "е6rт12ја8н  еонтваTЦвг2дoaкк|веву\n",
            "сaп'h  1К=јtнее вс.а.н'нсе АитеидН\n",
            "впќп кнл )каa| n\n",
            "ukиiртrl 1 енo неф1Wн\n",
            "Vкивси иа8таnиеിd 1цуоа3ч (каа\n",
            "пи7 Н |омп1в|=ез - rrвm 'р  н в1рлел 0 е Ж0 А\n",
            "3\\ >з    влеиквтb  хк  rмргидоќц.еп sпатеалу s =ллс гн0иДаwо p \n",
            ":ч,//(   _адuоtеiаее aа нс9eа i   иазW8лае|рF eпе  |а\n"
          ]
        }
      ],
      "source": [
        "prompt = 'како'\n",
        "context = torch.tensor(encode(prompt), dtype=torch.long, device=device)\n",
        "generated_chars = decode(m.generate(context.unsqueeze(0), max_new_tokens=500)[0].tolist())\n",
        "print(generated_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RL_GUsmTCsa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMnVtc0GXhWt2pH6vmCvtla",
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
